---
title: "R Notebook"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
  word_document: default
---

# Topic

Use twitteR package to load tweets with #cilantro and #coriander - a divisive topic.
Analyse sentiments using __ package

# Load packages etc.

```{r}
library(knitr)

opts_chunk$set(fig.width=6.7, dpi = 300, warning = FALSE, message = FALSE, echo = TRUE)

```

```{r}
library(devtools)
# install.packages("ROAuth")
# install.packages('httr')
# devtools::install_github("geoffjentry/twitteR")
library(twitteR)
library(ROAuth) #for authentication with twitter server
library(magrittr)
library(dplyr)
library(readr)
library(ggplot2)

#library of sentiments for words
# install.packages('syuzhet')
library(syuzhet)

```

# Create Twitter App

I found all these steps from [rStatistics.net](http://rstatistics.net/extracting-tweets-with-r/)

First, go to [https://apps.twitter.com/](https://apps.twitter.com/)

Set up an app
Click on Keys and Access Tokens tab
record for later tweet uses as file in *private* folder

Authenticate via R:
```{r}
keys <- read_csv('/Users/yam/OneDrive/shared files/Statslearningcourse/twitteR/keys.csv')

# setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)

setup_twitter_oauth(keys$key[1],
                    keys$key[2],
                    keys$key[3], 
                    keys$key[4])

```


### General Commands

Before I retrieve mine, some example retrieving commands below:

Tweets from a user or your own account:
```
userTimeline('r_programming',n=10) # tweets from a user

homeTimeline(n=15) # get tweets from home timeline

mentions(n=15) # get your tweets that were retweeted

favs <- favorites("r_programming", n =10) # tweets a user has favorited
```

Tweets with a particular hashtag 

```
tweets <- searchTwitter("rstats", n=25) # top 25 tweets that contain search term

tweetsDF <- twListToDF(tweets) # more info about tweets - converts to nice df
```

# Retrieve tweets

I want any tweets that contain 'coriander' or 'cilantro'. Cilantro is American and they can be pretty opinionated :p

### Back to the coriander problem:

Extract tweets and convert to a nice dataframe:

```{r}

tweets <- searchTwitter(c("coriander|cilantro"), 
                        n=800, lang = "en")
head(tweets)

# extract more info about tweets and convert to a nice df
tweetsDF <- twListToDF(tweets) 

tweetsDF <- tweetsDF %>% select(text, favorited, favoriteCount, created, retweetCount, retweeted, location, language, screenName)

str(tweetsDF)

```

Clean up the text column to remove hashtags, urls, and punctuation

```{r}
text.clean <- function(x) {

  x = gsub('http\\S+\\s*', '', x) ## Remove URLs

  x = gsub('\\b+RT', '', x) ## Remove RT

  x = gsub('#', '', x) ## Remove Hashtags

  x = gsub('@\\S+', '', x) ## Remove Mentions

  x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
  
  x = gsub('&amp', 'and', x) ## Remove Controls and special characters

  x = gsub('[,.:;+-]|\\[|\\]|\\/', ' ', x) ## space replaces some Punctuation
  
  x = gsub('[[:punct:]]', '', x) ## Remove Punctuations

  x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces

  x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces

  x = gsub(' +',' ',x) ## Remove extra whitespaces
  
  return(x)

}

#before cleaning
tweetsDF$text <- text.clean(tweetsDF$text)

# remove any tweets without cilantro or coriander
tweetsDF <- tweetsDF[grepl("cilantro|coriander", tweetsDF$text, ignore.case = TRUE),]

# use iconv function to convert character vector between encodings
tweetsDF$text <- iconv(tweetsDF$text, from = "latin1", to = "ASCII", sub = "byte")

# remove duplicates
tweetsDF <- tweetsDF[!duplicated(tweetsDF$text),]

#after cleaning
head(tweetsDF$text)

```


# Sentiment Analysis

I lifted this code from a blog by Julia Silge [http://juliasilge.com/blog/Joy-to-the-World/](http://juliasilge.com/blog/Joy-to-the-World/)

She uses the 'syuzhet' package developed in Stanford by Saif Mohammad and Peter Turney. They used the NRC Word-Emotion Association Lexicon to build a dictionary of words with scores for eight different emotions and two sentiments. Not every English word is included because most are neutral.

## Example Single Tweet

Let's look at an example tweet:

```{r}
get_nrc_sentiment("Sprinkling some Cilantro on what looks to be the perfect Gyro Gyro eatfresh eatlocal STL foodie eeeeeats")


token.sent <- function(x){
        output <- vector(mode = "list", length = length(x))
        
        for(i in seq_along(x)){
                by.word <- data_frame("token" = get_tokens(x[i]))
                by.word <- cbind(by.word, get_nrc_sentiment(by.word$token))
                #filter for rows with entries greater than 0:
                output[[i]] <- by.word[rowSums(by.word %>% select(-token)) != 0,]
        }
        
        names(output) = x
        return(output)
}

token.sent("Sprinkling some Cilantro on what looks to be the perfect Gyro Gyro eatfresh eatlocal STL foodie eeeeeats")

```

## Apply sentiment analysis to all tweets

<!-- Must first remove emoticons! -->

<!-- Lifted from here: -->
<!-- http://opiateforthemass.es/articles/emoticons-in-R/ -->

<!-- ### First work on one tweet -->

```{r, include = FALSE, eval = FALSE}
sents = get_nrc_sentiment(tweetsDF$text)
#doesn't work because emoticons are not printed as their actual UTF8 encodings

emoticon.ex <- tweetsDF$text[grep("well now u know one person", tweetsDF$text)]
emoticon.ex

# use iconv function to convert character vector between encodings
emoticon.ex <- iconv(emoticon.ex, from = "latin1", to = "ASCII", sub = "byte")

# load emoticon decoder csv file
decoder <- read_csv("emoticons.csv") %>% select(Description, encoding)
decoder

# emoticon.ex <- apply(decoder, 1, function(x) gsub(x[2], x[1], emoticon.ex))



```

<!-- ### Fix emoticons in all tweets -->

```{r, include = FALSE, eval = FALSE}

sents = get_nrc_sentiment(tweetsDF$text)
#doesn't work because emoticons are not printed as their actual UTF8 encodings

# use iconv function to convert character vector between encodings
tweetsDF$text <- iconv(tweetsDF$text, from = "latin1", to = "ASCII", sub = "byte")

# remove duplicates
tweetsDF <- tweetsDF[!duplicated(tweetsDF$text),]
```


```{r}
sents = get_nrc_sentiment(tweetsDF$text)

tweetsDF <- cbind(tweetsDF, sents)


```

## Investigate

Highest sentiment tweets

```{r}

sent.summary <- function(sentiment = "positive", df = tweetsDF, n = 4){
        temp <- df[order(-df[,sentiment]),] %>% slice(1:n)
        print(
                token.sent(temp$text)
        )
}

sent.summary("disgust")
sent.summary("anger")
sent.summary("joy")


```

## Make some plots

```{r}

sentimentTotals <- data.frame(colSums(tweetsDF[,c(10:19)]))
names(sentimentTotals) <- "count"
sentimentTotals <- cbind("sentiment" = rownames(sentimentTotals), sentimentTotals)
rownames(sentimentTotals) <- NULL
ggplot(data = sentimentTotals, aes(x = sentiment, y = count)) +
        geom_bar(aes(fill = sentiment), stat = "identity") +
        theme(legend.position = "none") +
        xlab("Sentiment") + ylab("Total Count") + ggtitle("Total Sentiment Score for All Tweets")
```


