---
title: "Intro to Twitter Sentiment Analysis"
output:
  html_document: default
  html_notebook: default
  pdf_document: default
  word_document: default
---

# Overview

Use twitteR package to load tweets with #cilantro and #coriander - a divisive topic.
Analyse sentiments using 3 new packages and 5 new functions:

* Connect to Twitter: `ROAuth`
    * `setup_twitter_oauth()`
* Scrape Twitter: `twitteR`
    * `searchTwitter()`
    * `twListToDF()`
* Sentiment analysis: `syuzhet`
    * `get_nrc_sentiment()`
    * `get_tokens()`
    
Some knowledge of regular expressions will also help, but only base functions are used.

# Load packages

```{r, include = FALSE}
library(knitr)

opts_chunk$set(fig.width=6.7, dpi = 300, warning = FALSE, message = FALSE, echo = TRUE)

```

```{r}
library(devtools)
# install.packages("ROAuth")
# install.packages('httr')
# devtools::install_github("geoffjentry/twitteR")
library(twitteR)
library(ROAuth) #for authentication with twitter server
library(magrittr)
library(dplyr)
library(readr)
library(ggplot2)

#library of sentiments for words
# install.packages('syuzhet')
library(syuzhet)

```

## Create Twitter App and connect inside R

* These steps are from [rStatistics.net](http://rstatistics.net/extracting-tweets-with-r/)
* First, go to [https://apps.twitter.com/](https://apps.twitter.com/).
* Set up an app
* Click on Keys and Access Tokens tab
* record for later tweet uses as file in *private* folder
* Authenticate via R

This step uses only one function from the `ROAuth` package:
* `setup_twitter_oauth()`

```{r}
if(.Platform$OS.type == 'windows'){
        keys <- read_csv('C:/Users/n9232371/OneDrive/shared files/Statslearningcourse/twitteR/keys.csv')

} else{
        keys <- read_csv('/Users/yam/OneDrive/shared files/Statslearningcourse/twitteR/keys.csv')

}

# setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)

setup_twitter_oauth(keys$key[1],
                    keys$key[2],
                    keys$key[3], 
                    keys$key[4])

```


## General Commands for `twitteR` package


Tweets from a user or your own account:
```
userTimeline('r_programmer',n=10) # tweets from a user

homeTimeline(n=15) # get tweets from your home timeline

mentions(n=15) # get your tweets that were retweeted

favs <- favorites("r_programmer", n =10) # tweets a user has favorited
```

Tweets with a particular hashtag or word

```
tweets <- searchTwitter("rstats", n=25) # top 25 tweets that contain search term

tweetsDF <- twListToDF(tweets) # more info about tweets - converts to nice df
```

## Scrape Tweets and Clean

I want any tweets that contain 'coriander' or 'cilantro'. Cilantro is American for coriander :)

Extract tweets and convert to a nice dataframe using only two functions from `twitteR`:

* `searchTwitter()`
* `twListToDF()`

```{r}

tweets <- searchTwitter(c("coriander|cilantro"), 
                        n=800, lang = "en")
head(tweets, 3)

# extract more info about tweets and convert to a nice df
tweetsDF <- twListToDF(tweets) 

tweetsDF <- tweetsDF %>% select(text, favorited, favoriteCount, created, retweetCount, retweeted, screenName)

str(tweetsDF)

```

Clean up the text column to remove hashtags, urls, and punctuation.
This section uses base functions:

* `gsub()`
* `iconv()`
* `grepl()`

```{r}
text.clean <- function(x) {

  x = gsub('http\\S+\\s*', '', x) ## Remove URLs

  x = gsub('\\b+RT', '', x) ## Remove RT

  x = gsub('#', '', x) ## Remove Hashtags

  x = gsub('@\\S+', '', x) ## Remove Mentions

  x = gsub('[[:cntrl:]]', '', x) ## Remove Controls and special characters
  
  x = gsub('&amp', 'and', x) ## Remove Controls and special characters

  x = gsub('[,.:;+-]|\\[|\\]|\\/', ' ', x) ## space replaces some Punctuation
  
  x = gsub('[[:punct:]]', '', x) ## Remove Punctuations

  x = gsub("^[[:space:]]*","",x) ## Remove leading whitespaces

  x = gsub("[[:space:]]*$","",x) ## Remove trailing whitespaces

  x = gsub(' +',' ',x) ## Remove extra whitespaces
  
  return(x)

}


tweetsDF$text <- text.clean(tweetsDF$text)

# use iconv function to convert character vector between encodings
tweetsDF$text <- iconv(tweetsDF$text, from = "latin1", to = "ASCII", sub = "byte")

# insert space before '<' and after '>' if it is next to a word
tweetsDF$text  <- gsub('([[:alpha:]])([<])', '\\1 \\2', tweetsDF$text )
tweetsDF$text  <- gsub('([>])([[:alpha:]])', '\\1 \\2', tweetsDF$text )
  
# remove any tweets without cilantro or coriander
tweetsDF <- tweetsDF[grepl("cilantro|coriander", tweetsDF$text, ignore.case = TRUE),]

# remove duplicates
tweetsDF <- tweetsDF[!duplicated(tweetsDF$text),]

#after cleaning
head(tweetsDF$text)

```


## Sentiment Analysis

I lifted this code from a blog by Julia Silge [http://juliasilge.com/blog/Joy-to-the-World/](http://juliasilge.com/blog/Joy-to-the-World/)

She uses the 'syuzhet' package developed in Stanford by Saif Mohammad and Peter Turney. They used the NRC Word-Emotion Association Lexicon to build a dictionary of words with scores for eight different emotions and two sentiments. Not every English word is included because most are neutral.

We will use the following functions from the `syuzhet` package:

* `get_nrc_sentiment()`
* `get_tokens()`

### Example Single Tweet


```{r}
get_nrc_sentiment("Tonights dinner menu my famous green chili lemon cilantro chicken enchiladas Time to roll some fat ones")


token.sent <- function(x){
        output <- vector(mode = "list", length = length(x))
        
        for(i in seq_along(x)){
                by.word <- data_frame("token" = get_tokens(x[i]))
                by.word <- cbind(by.word, get_nrc_sentiment(by.word$token))
                #filter for rows with entries greater than 0:
                output[[i]] <- by.word[rowSums(by.word %>% select(-token)) != 0,]
        }
        
        names(output) = x
        return(output)
}

token.sent("Tonights dinner menu my famous green chili lemon cilantro chicken enchiladas Time to roll some fat ones")

```

## Apply sentiment analysis to all tweets


```{r}
sents = get_nrc_sentiment(tweetsDF$text)

tweetsDF <- cbind(tweetsDF, sents)


```

## Make a plot

```{r}

sents <- c('negative', 'anger','disgust','sadness','anticipation','surprise','trust','joy','positive')

plot.df <- data.frame("count" = colSums(tweetsDF[, sents]),
                              "sentiment" = sents)
#reorder levels
plot.df$sentiment <- factor(plot.df$sentiment, levels= sents)

ggplot(plot.df, aes(x = sentiment, y = count)) +
        geom_bar(aes(fill = sentiment), stat = "identity") +
        theme(legend.position = "none") +
        xlab("") + ylab("Total Count") + ggtitle("Total Sentiment Score for All Tweets") +
        scale_fill_brewer(palette = "Spectral")



```



Compare to AFINN method of sentiment classification

```{r}

#trial on single sentence

x = 'Wow 2 awards this year for our Coppa and Fennel and Coriander Salami Thanks Great Taste Awards'
        
get_sentiment(x, method = 'afinn')

token.afinn <- function(x){
        output <- vector(mode = "list", length = length(x))
        
        for(i in seq_along(x)){
                by.word <- data_frame("token" = c(get_tokens(x[i])))
                
                by.word <- cbind(by.word, 'afinn' = get_sentiment(by.word$token, method = 'afinn'))
                total <- data_frame('token' = c("TOTAL"),
                                    'afinn' = get_sentiment(x[i], method = 'afinn'))
                
                by.word <- rbind(by.word, total)
                
                #filter for rows with entries greater than 0:
                output[[i]] <- by.word[rowSums(by.word %>% select(-token)) != 0,]
        }
        
        names(output) = x
        return(output)
}

token.afinn(x)

# run full dataset through

tweetsDF <- cbind(tweetsDF,
                  'afinn' = get_sentiment(tweetsDF$text, method = 'afinn'))


```

## Investigate

Highest sentiment tweets

```{r}

sent.summary <- function(sentiment = "positive", df = tweetsDF, n = 3, order.desc = TRUE){
        if(order.desc == TRUE){
        temp <- df[order(-df[,sentiment]),] %>% slice(1:n)
        } else {
                temp <- df[order(df[,sentiment]),] %>% slice(1:n)
        }
        
        if(sentiment != "afinn"){
        top.n <- token.sent(temp$text)
        } else {
                top.n = token.afinn(temp$text)
        }
        
        print(temp[,c(sentiment)])
        print(
                top.n
        )
        
}

sent.summary("disgust")
sent.summary("anticipation")
sent.summary("retweetCount")
sent.summary("favoriteCount")
sent.summary("afinn", n = 6)
sent.summary("afinn", order.desc = FALSE, n = 20)


```



Test new package

```{r}

hash_sentiment(regex = "coriander|cilantro", num.tweets = 200, 
               method = "afinn", sentiment = "negative",
               num.summary = 7, scrape = FALSE)

hash_sentiment(regex = "coriander|cilantro", num.tweets = 200, 
               method = "afinn", sentiment = "positive",
               num.summary = 6, scrape = FALSE)

hash_sentiment(regex = "taylor swift", num.tweets = 400, 
               method = "afinn", sentiment = "negative",
               num.summary = 7, scrape = TRUE)

hash_sentiment(regex = "taylor swift", num.tweets = 400, 
               method = "afinn", sentiment = "positive",
               num.summary = 6, scrape = FALSE)

tweetsDF %>% arrange(desc(favoriteCount)) %>% head(6) %>% .$text
tweetsDF %>% arrange(desc(retweetCount)) %>% head(6) %>% .$text
        

```



